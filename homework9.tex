\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework\ \#9}
\newcommand{\hmwkDueDate}{Nov 28, 2022}
\newcommand{\hmwkClass}{Matrix Calculation}
\newcommand{\hmwkClassTime}{Monday}
\newcommand{\hmwkClassInstructor}{Professor Jun Lai}
\newcommand{\hmwkAuthorName}{\textbf{Shuang Hu}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 3:10pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
\newcommand{\norm}[1]{\|#1\|}
% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\supp}{\text{supp}}
\newcommand{\Rn}{\mathbb{R}^{n}}
\newcommand{\dif}{\mathrm{d}}
\newcommand{\avg}[1]{\left\langle #1 \right\rangle}
\newcommand{\difFrac}[2]{\frac{\dif #1}{\dif #2}}
\newcommand{\pdfFrac}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\OFL}{\mathrm{OFL}}
\newcommand{\UFL}{\mathrm{UFL}}
\newcommand{\fl}{\mathrm{fl}}
\newcommand{\op}{\odot}
\newcommand{\cp}{\cdot}
\newcommand{\Eabs}{E_{\mathrm{abs}}}
\newcommand{\Erel}{E_{\mathrm{rel}}}
\newcommand{\DR}{\mathcal{D}_{\widetilde{LN}}^{n}}
\newcommand{\add}[2]{\sum_{#1=1}^{#2}}
\newcommand{\innerprod}[2]{\left<#1,#2\right>}
\newcommand\tbbint{{-\mkern -16mu\int}}
\newcommand\tbint{{\mathchar '26\mkern -14mu\int}}
\newcommand\dbbint{{-\mkern -19mu\int}}
\newcommand\dbint{{\mathchar '26\mkern -18mu\int}}
\newcommand\bint{
{\mathchoice{\dbint}{\tbint}{\tbint}{\tbint}}
}
\newcommand\bbint{
{\mathchoice{\dbbint}{\tbbint}{\tbbint}{\tbbint}}
}
\begin{document}
\maketitle
\pagebreak
\begin{homeworkProblem}
    (P355 Problem 7.1.3)

    Assume that $Q=\begin{bmatrix}
        q_{1}&\cdots&q_{n}\\
    \end{bmatrix}$, by the definition of Schur decomposition, we can see that
    \begin{equation}
        \label{eq:1}
        A\begin{bmatrix}
            q_{1}&\cdots&q_{n}
        \end{bmatrix}=\begin{bmatrix}
            q_{1}&\cdots&q_{n}
        \end{bmatrix}T.
    \end{equation}
    while 
    \begin{equation}
        T:=\begin{bmatrix}
            \lambda_{1}&t_{12}&\cdots&t_{1n}\\
            &\lambda_{2}&\cdots&t_{2n}\\
            &&\ddots&\vdots&\\
            &&&\lambda_{n}&\\
        \end{bmatrix}.
    \end{equation}
    As matrix $A$ has distinct eigenvalues, we can see: $\lambda_{i}\neq\lambda_{j},\forall i\neq j$. Then we suffice to show that: $\forall m\le n$, $span\{q_{1},\cdots,q_{m}\}$ is a B-invariant subspace.

    Then, we prove this result by induction. First, for $m=1$, we can see that $Aq_{1}=\lambda_{1}q_{1}$, and $AB=BA$ means $ABq_{1}=BAq_{1}=\lambda_{1}Bq_{1}$, which means that $Bq_{1}$ belongs to the eigen-subspace of $A$ related to eigenvalue $\lambda_{1}$. So $Bq_{1}=\mu_{1}q_{1}$. Assume $span\{q_{1},\cdots,q_{t}\}$ is a B-invariant subspace with $t\le m$, consider the subspace $V_{m+1}=span\{q_{1},\cdots,q_{m+1}\}$. By \eqref{eq:1}, we can see:
    \begin{equation}
        \label{eq:Aqm+1}
        Aq_{m+1}=\sum_{i=1}^{m}t_{i,m+1}q_{i}+\lambda_{m+1}q_{m+1}.
    \end{equation}
    As $AB=BA$, we can see:
    \begin{equation}
        ABq_{m+1}=BAq_{m+1}=B(\sum_{i=1}^{m}t_{i,m+1}q_{i}+\lambda_{m+1}q_{m+1}).
    \end{equation}

    It means:
    \begin{equation}
        (A-\lambda_{m+1}I)Bq_{m+1}=B(\sum_{i=1}^{m}t_{i,m+1}q_{i}).
    \end{equation}

    If $Bq_{m+1}\notin span\{q_{1},\cdots,q_{m+1}\}$, i.e. $Bq_{m+1}=\sum_{i=1}^{n} \alpha_{i}q_{i}$ while $\exists i>m+1$, $\alpha_{i}\neq 0$, we can see:
    \begin{equation}
        (A-\lambda_{m+1}I)Bq_{m+1}=v+\sum_{i=m+2}^{n}(\lambda_{i}-\lambda_{m+1})\alpha_{i}q_{i}.
    \end{equation}
    while $v\in V_{m+1}$. Meanwhile:
    \begin{equation}
        B(\sum_{i=1}^{m}t_{i,m+1}q_{i})\in V_{m}.
    \end{equation}
    Contradict! So $span\{q_{1},\cdots,q_{m+1}\}$ is an invariant subspace as well.

    By induction, $Q^{H}BQ$ is also an upper-triangular matrix.
\end{homeworkProblem}
\begin{homeworkProblem}
    (P355 Problem 7.1.5)

    By Schur decomposition, exists $Q\in\mathbb{C}^{n\times n}$ such that:
    \begin{equation}
        Q^{H}AQ=T
    \end{equation}
    as $T$ upper-triangular. If $T$ has distinct eigenvalues, we just set $B=A$, $B$ is diagonalizable actually. If there exists $i,j$ such that $\lambda_{i}=\lambda_{j}$, we set diagonal matrix 
    \begin{equation}
        D=\text{diag}\{d_{1},\cdots,d_{n}\}.
    \end{equation}
    $d_{i}$ satisfies the following conditions:
    \begin{itemize}
        \item $|d_{i}|\le\frac{\epsilon}{n}.$
        \item $\lambda_{i}+d_{i}\neq \lambda_{j}+d_{j}$.
    \end{itemize}
    As $n<\infty$, such matrix $D$ exists. We set 
    \begin{equation}
        B=Q(T+D)Q^{H},
    \end{equation}
    then:
    \begin{equation}
        \|A-B\|_{2}\le\|D\|_{2}\le\epsilon.
    \end{equation}
    As the eigenvalues of $B$ are distinct, $B$ is diagonalizable. So $B$ is what we want to see.
\end{homeworkProblem}
\begin{homeworkProblem}
    (Page 374, Problem 7.3.7)

    (a)
    As $\|\cdot\|_{2}$ is a norm, we can derive $\|\cdot\|_{X}$ is a norm directly.

    Then, as $\|\cdot\|_{2}$ is a matrix norm, we can see:
    \begin{equation}
        \|AB\|_{X}=\|X^{-1}AXX^{-1}BX\|_{2}\le\|X^{-1}AX\|_{2}\|X^{-1}BX\|_{2}=\|A\|_{X}\|B\|_{X}.
    \end{equation}

    (b)
    Assume the Schur decomposition of matrix $A$ is:
    \begin{equation}
        Q^{H}AQ=D+N.
    \end{equation}
    Assume $D_{a}=\text{diag}\{1,a,\cdots,a^{n-1}\}$, set $X=QD_{a}$, we can see that:
    \begin{equation}
        X^{-1}AX=D_{a}^{-1}(D+N)D_{a}=D+D_{a}^{-1}ND_{a}.
    \end{equation}
    We can set $a$ such that 
    \begin{equation}
        \|D_{a}^{-1}ND_{a}\|\le\epsilon.
    \end{equation}
    Then:
    \begin{equation}
        \|X^{-1}AX\|_{2}\le\|D+D_{a}^{-1}ND_{a}\|_{2}\le\rho(D)+\epsilon=\rho(A)+\epsilon.
    \end{equation}

    For given matrix $X$, $\|\cdot\|_{2}$ is equivalent with $\|\cdot\|_{X}$. So:
    \begin{equation}
        \|A^{k}\|_{2}\le M\|A^{k}\|_{X}\le M(\rho(A)+\epsilon)^{k}.
    \end{equation}
\end{homeworkProblem}
\end{document}